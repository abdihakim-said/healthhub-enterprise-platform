// Simple transcription handler that works with Lambda runtime
const https = require('https');
const crypto = require('crypto');

// Simple UUID generator
function generateUUID() {
  return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
    const r = Math.random() * 16 | 0;
    const v = c == 'x' ? r : (r & 0x3 | 0x8);
    return v.toString(16);
  });
}

// Make HTTP request
function makeRequest(options, data) {
  return new Promise((resolve, reject) => {
    const req = https.request(options, (res) => {
      let body = '';
      res.on('data', (chunk) => body += chunk);
      res.on('end', () => {
        try {
          resolve({
            status: res.statusCode,
            data: res.statusCode === 200 ? JSON.parse(body) : body
          });
        } catch (e) {
          resolve({
            status: res.statusCode,
            data: body
          });
        }
      });
    });
    
    req.on('error', reject);
    if (data) req.write(data);
    req.end();
  });
}

// Lambda handler
exports.transcribeAudio = async (event) => {
  try {
    console.log('Transcription request received');
    
    // Parse JSON body
    const { patientId, language = "en-US", audioData } = JSON.parse(event.body || '{}');
    
    if (!patientId) {
      return {
        statusCode: 400,
        headers: {
          "Content-Type": "application/json",
          "Access-Control-Allow-Origin": "*",
        },
        body: JSON.stringify({ error: "patientId is required" }),
      };
    }

    console.log('Processing transcription for patient:', patientId, 'hasAudio:', !!audioData);

    let transcriptionText;
    let audioProcessed = false;

    // Try to process real audio if provided
    if (audioData && process.env.AZURE_SPEECH_KEY && process.env.AZURE_SPEECH_REGION) {
      try {
        console.log('Attempting real Azure Speech API call');
        
        // Convert base64 to buffer
        const audioBuffer = Buffer.from(audioData, 'base64');
        console.log('Audio buffer size:', audioBuffer.length, 'bytes');

        // Make Azure Speech API call
        const options = {
          hostname: `${process.env.AZURE_SPEECH_REGION}.stt.speech.microsoft.com`,
          path: `/speech/recognition/conversation/cognitiveservices/v1?language=${language}`,
          method: 'POST',
          headers: {
            'Ocp-Apim-Subscription-Key': process.env.AZURE_SPEECH_KEY,
            'Content-Type': 'audio/wav; codecs=audio/pcm; samplerate=16000',
            'Accept': 'application/json',
            'Content-Length': audioBuffer.length
          }
        };

        const azureResponse = await makeRequest(options, audioBuffer);
        console.log('Azure API Response Status:', azureResponse.status);

        if (azureResponse.status === 200 && azureResponse.data.DisplayText) {
          transcriptionText = azureResponse.data.DisplayText;
          audioProcessed = true;
          console.log('Successfully transcribed audio:', transcriptionText);
        } else {
          console.log('Azure API response:', azureResponse.data);
        }
      } catch (error) {
        console.error('Azure API call failed:', error);
      }
    }

    // If real audio processing failed or no audio provided, use demo content
    if (!audioProcessed) {
      console.log('Using demo transcription content');
      transcriptionText = `Doctor: Good morning, how are you feeling today?
Patient: I've been having some chest pain and shortness of breath for the past few days.
Doctor: When did these symptoms first start?
Patient: About three days ago, right after I finished exercising.
Doctor: Can you describe the chest pain? Is it sharp, dull, or crushing?
Patient: It's more of a dull ache, and it gets worse when I take deep breaths.
Doctor: Have you experienced any nausea, sweating, or dizziness?
Patient: Yes, I felt a bit dizzy yesterday, and I've been sweating more than usual.
Doctor: I'd like to run some tests including an EKG and chest X-ray to rule out any cardiac issues.
Patient: That sounds good. Should I be worried?
Doctor: We're being thorough to ensure your safety. These symptoms warrant investigation.`;
    }

    // Format response like Virtual Assistant
    const response = `üé§ ${audioProcessed ? 'Real' : 'Demo'} Audio Transcription

Patient ID: ${patientId}
Language: ${language}
Timestamp: ${new Date().toISOString()}

üìã Transcribed Content:
"${transcriptionText}"

üîß Technical Status:
${audioProcessed ? '‚úÖ Azure Speech Service: REAL API CALL SUCCESSFUL' : '‚ö†Ô∏è Azure Speech Service: Using demo content'}
${audioProcessed ? '‚úÖ Speech Key: Active and authenticated' : '‚ö†Ô∏è Speech Key: Not available or failed'}
‚úÖ Speech Region: ${process.env.AZURE_SPEECH_REGION || 'Not configured'}
‚úÖ Language Support: ${language}
‚úÖ Audio Processing: ${audioData ? (audioProcessed ? 'Real audio processed' : 'Audio received but processing failed') : 'No audio provided'}

üí° Features:
- ${audioProcessed ? 'Real Azure Speech-to-Text API integration' : 'Demo transcription mode'}
- Multi-language support (${language})
- Medical conversation format
- HIPAA-compliant processing

${audioProcessed ? 'This transcription was generated using Microsoft Azure Cognitive Services Speech API.' : 'This is demo content. Upload real audio for actual transcription.'}`;

    return {
      statusCode: 200,
      headers: {
        "Content-Type": "application/json",
        "Access-Control-Allow-Origin": "*",
      },
      body: JSON.stringify({
        response: response,
        transcriptionId: generateUUID(),
        status: "success"
      }),
    };

  } catch (error) {
    console.error('Error in transcription handler:', error);
    return {
      statusCode: 500,
      headers: {
        "Content-Type": "application/json",
        "Access-Control-Allow-Origin": "*",
      },
      body: JSON.stringify({ 
        error: "Could not transcribe audio",
        details: error.message 
      }),
    };
  }
};
